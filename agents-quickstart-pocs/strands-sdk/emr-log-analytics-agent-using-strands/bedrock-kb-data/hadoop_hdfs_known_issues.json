{"id":"hdfs-1001","summary":"Blocks Were Missing or Could Not Obtain Block","description":"This may be caused due to data node failures, replication issues, accidental deletions, or because of a faulty NameNode switch. Hadoop tools such as 'hdfs fsck' can be used to check the integrity of the file system. Amazon CloudWatch metrics can be monitored to understand when corrupt or missing blocks started to happen, and HDFS logs can be explored during that timeline.","keywords":["org.apache.hadoop.hdfs.BlockMissingException"],"knowledge_center_links":[""]}
{"id":"hdfs-1002","summary":"HDFS NameNode Went into Safe Mode","description":"Safe mode for the NameNode is a read-only mode for the Hadoop Distributed File System (HDFS) cluster. In safe mode, you can't make any modifications to the file system or blocks. Check why the NameNode went into safe mode. Ideally, it should come out of safe mode when the configured criteria for exiting safe mode is met (example: replaying edit logs). If the NameNode is stuck in safe mode for an extended period, you will need to check the logs to understand the reasons.","keywords":["org.apache.hadoop.hdfs.server.namenode.SafeModeException"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-namenode-turn-off-safemode"]}
{"id":"hdfs-1003","summary":"NameNode Stuck in a Loop While Replicating a Non-Existent Block/File","description":"The NameNode sometimes gets stuck in a loop replicating when there is an abrupt exit or shutdown of the client who is copying the file, or suddenly the NameNode would switch to safe mode stating no blocks can be written to HDFS.","keywords":["org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException"],"knowledge_center_links":[""]}
{"id":"hdfs-1004","summary":"NameNode Entering into Safe Mode","description":"When the NameNode enters into safe mode, this exception will be seen. We recommend checking the NameNode logs to see the reason for the NameNode going into safe mode.","keywords":["org.apache.hadoop.hdfs.server.namenode.FSNamesystem.newSafemodeException"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-namenode-turn-off-safemode"]}
{"id":"hdfs-1005","summary":"NameNode Could Not Communicate with DataNode","description":"If there is any communication problem between the NameNode and a DataNode, then you typically see this exception, and if we have only one DataNode, then the NameNode will not come up.","keywords":["org.apache.hadoop.hdfs.server.protocol.DisallowedDatanodeException"],"knowledge_center_links":[""]}
{"id":"hdfs-1006","summary":"NameNode Storage Directory is Inaccessible or Inconsistent","description":"org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /mnt/namenode is in an inconsistent state: storage directory does not exist or is not accessible.","keywords":["org.apache.hadoop.hdfs.server.common.InconsistentFSStateException"],"knowledge_center_links":[""]}
{"id":"hdfs-1007","summary":"File is Still Being Written to HDFS","description":"You see 'Cannot obtain block length for LocatedBlock', which means the file is still in the being-written state, i.e., it has not been closed yet, and the reader cannot successfully identify its current length by communicating with the corresponding DataNodes.","keywords":["org.apache.hadoop.hdfs.CannotObtainBlockLengthException"],"knowledge_center_links":[""]}
{"id":"hdfs-1008","summary":"Could Not Replicate the Ingested Block or File","description":"When trying to write a file at that time, if it could not replicate, we typically see this exception.","keywords":["org.apache.hadoop.hdfs.server.namenode.NotReplicatedYetException"],"knowledge_center_links":[""]}
{"id":"hdfs-1009","summary":"JournalNodeManager Could Not Respond","description":"If there is any issue with the JournalManager, then we will get this exception.","keywords":["org.apache.hadoop.hdfs.qjournal.client.QuorumException"],"knowledge_center_links":[""]}
{"id":"hdfs-1010","summary":"Replica Missing from the Respective Block Pool","description":"Generally, there could be two possible reasons: ReplicaNotFound or BlockMissing.","keywords":["org.apache.hadoop.hdfs.server.datanode.ReplicaNotFoundException"],"knowledge_center_links":[""]}
{"id":"hdfs-1011","summary":"NameNode Could Not Get the Lease on the File to Write","description":"While data is being written to a file and is not able to write or get the lease on that file to perform write operations.","keywords":["org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException"],"knowledge_center_links":[""]}
{"id":"hdfs-1012","summary":"Filesystem Has Reached the Limit","description":"Occurs when a directory has reached the maximum number of files in the filesystem with the error FSLimitException.","keywords":["org.apache.hadoop.hdfs.protocol.FSLimitException"],"knowledge_center_links":[""]}
{"id":"hdfs-1013","summary":"Blocks Already Exist","description":"This indicates that the target block already exists and is not set to be recovered or overwritten.","keywords":["org.apache.hadoop.hdfs.server.datanode.ReplicaAlreadyExistsException"],"knowledge_center_links":[""]}
{"id":"hdfs-1014","summary":"Append Operation for an Existing File or Block","description":"This indicates the NameNode is currently in the process of recovering from a failure or undergoing some maintenance.","keywords":["org.apache.hadoop.hdfs.protocol.RecoveryInProgressException"],"knowledge_center_links":[""]}
{"id":"hdfs-1015","summary":"Quorum Exception Where It Could Not Create","description":"When the JournalNodes could not create a quorum.","keywords":["org.apache.hadoop.hdfs.qjournal.client.QuorumException.create"],"knowledge_center_links":[""]}
{"id":"hdfs-1016","summary":"NameNode Could Not Replay the Edit Log","description":"If there is any mismatch between the current log transaction ID and the future transaction ID, then we will see an exception like (Error replaying edit log at offset 0. Expected transaction ID was XXXXXXX).","keywords":["org.apache.hadoop.hdfs.server.namenode.EditLogInputException"],"knowledge_center_links":[""]}
{"id":"hdfs-1017","summary":"When JournalNode is Not Able to Join Quorum or Has an Issue with Any Node","description":"The call to form a quorum fails due to an issue with the process or with the node.","keywords":["org.apache.hadoop.hdfs.qjournal.client.QuorumCall.rethrowException"],"knowledge_center_links":[""]}