{"id":"hbase-1001","summary":"The Error Occurs When an RPC Operation Fails After Retrying for hbase.client.retries.number","description":"Exception thrown by HTable methods when an attempt to do something (like commit changes) fails after a bunch of retries. There could be multiple factors for such errors - connection issues, regions in transition. You will need to look into the HBase master and respective region server logs.","keywords":["org.apache.hadoop.hbase.client.RetriesExhaustedException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/client/RetriesExhaustedException.html?is-external=true"]}
{"id":"hbase-1002","summary":"Phoenix StaleRegionBoundaryCacheException Detected","description":"Phoenix tries to connect to the hbase:meta table by default. However, because the hbase:meta table belongs to the primary cluster, Phoenix cannot connect to the read-replica cluster. To resolve this problem, modify hbase-site.xml to point to the hbase:meta_cluster-id table that belongs to the HBase read-replica cluster.","keywords":["StaleRegionBoundaryCacheException","Cache of region boundaries are out of date"],"knowledge_center_links":[""]}
{"id":"hbase-1003","summary":"The Error Occurs When the Master Has Not Completed Its Initialization","description":"This exception is thrown by the master when a region server was shut down and restarted so fast that the master still hasn't processed the server shutdown of the first instance, or when the master is initializing and a client calls admin operations, or when an operation is performed on a region server that is still starting. Please check the master logs to see where the initialization is stuck.","keywords":["org.apache.hadoop.hbase.PleaseHoldException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/PleaseHoldException.html"]}
{"id":"hbase-1004","summary":"The Error Occurs on the Client Side If the Region Where the Request Is Made Is Not Serving the Data for the Specific Region","description":"Thrown by a region server if it is sent a request for a region it is not serving. Generally, this happens when a region is not online or is undergoing a split. You need to check for stuck procedures for the region and bypass them. In cases like this, the master moves the region to the OFFLINE state and re-assigns it to a different RegionServer. Please retry your operation. If the NotServingRegionException is logged at the ERROR level, it means RetriesExhausted, and the region's health would need to be verified.","keywords":["org.apache.hadoop.hbase.NotServingRegionException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/NotServingRegionException.html"]}
{"id":"hbase-1005","summary":"This Exception Depicts a Client-Side Timeout","description":"Client-side call timeout. Try checking the Region Server logs mentioned in the error. Some possible causes could be exceptions like CallQueueTooBigException at the Region Server logs. Increasing timeouts can also be helpful.","keywords":["org.apache.hadoop.hbase.ipc.CallTimeoutException"],"knowledge_center_links":"https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/ipc/CallTimeoutException.html"}
{"id":"hbase-1006","summary":"HBase RemoteWithExtrasException Detected","description":"A RemoteException with some extra information. If the source exception was a DoNotRetryIOException, isDoNotRetry() will return true. A RemoteException hosts exceptions we got from the server. You will need to look at the remote exception. The exception would carry details of another exception that is occurring. The exception provides details of the error coming from the remote host.","keywords":["org.apache.hadoop.hbase.ipc.RemoteWithExtrasException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/ipc/RemoteWithExtrasException.html"]}
{"id":"hbase-1007","summary":"HBase RetriesExhaustedWithDetailsException Provides Details on the Rowkey Giving a Problem","description":"This subclass of RetriesExhaustedException is thrown when we have more information about which rows were causing which exceptions on what servers. You can call mayHaveClusterIssues(), and if the result is false, you have input error problems; otherwise, you may have cluster issues. You can iterate over the causes, rows, and last known server addresses via getNumExceptions(), getCause(int), getRow(int), and getHostnamePort(int).","keywords":["org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/index.html?org/apache/hadoop/hbase/client/RetriesExhaustedWithDetailsException.html"]}
{"id":"hbase-1008","summary":"Exception 'ServerNotRunningYetException: Server Is Not Running Yet' Generally Occurs When the Master Is Not Running or Initialized","description":"Exception 'ServerNotRunningYetException: Server is not running yet' generally occurs when the master is not running or initialized. Please check the HBase master logs to confirm the actual root cause. This could be due to existing system procedures not being completed. Please check at which point the exception is coming.","keywords":["org.apache.hadoop.hbase.ipc.ServerNotRunningYetException","Server is not running yet"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/ipc/ServerNotRunningYetException.html"]}
{"id":"hbase-1009","summary":"Thrown When Replay of WAL Is Required Because a Snapshot Was Not Properly Persisted","description":"Thrown during a flush if the possibility exists that snapshot content was not properly persisted into store files. The response should include a replay of the WAL content. The region is put into closing mode, and the caller MUST abort after this.","keywords":["org.apache.hadoop.hbase.DroppedSnapshotException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/DroppedSnapshotException.html"]}
{"id":"hbase-1010","summary":"org.apache.hadoop.hbase.DoNotRetryIOException, Please Check for the Accompanying Exception","description":"Thrown during an exception, and all other exceptions are subclasses of this parent class. Please check the exception, and initiate a retry of the same.","keywords":["org.apache.hadoop.hbase.DoNotRetryIOException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/index.html?org/apache/hadoop/hbase/DoNotRetryIOException.html"]}
{"id":"hbase-1011","summary":"HBase HBaseIOException Detected","description":"All HBase-specific IOExceptions are part of this class. Please check the corresponding exception being generated to verify the root cause.","keywords":["org.apache.hadoop.hbase.HBaseIOException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/class-use/HBaseIOException.html"]}
{"id":"hbase-1012","summary":"org.apache.hadoop.hbase.io.hfile.CorruptHFileException: The Underlying HFile May Be Corrupted","description":"This exception is thrown when attempts to read an HFile fail due to corruption or truncation issues. Please check if the HFile is corrupted or not using the HFile tool (https://hbase.apache.org/book.html#hfile_tool).","keywords":["org.apache.hadoop.hbase.io.hfile.CorruptHFileException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/CorruptHFileException.html"]}
{"id":"hbase-1013","summary":"org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException: Column Family Does Not Exist in Region","description":"The exception is thrown when accessing a column family that does not exist for a table.","keywords":["org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/NoSuchColumnFamilyException.html"]}
{"id":"hbase-1014","summary":"HRegionServer Aborted","description":"The exception is thrown by the region server when it is aborting.","keywords":["org.apache.hadoop.hbase.regionserver.RegionServerAbortedException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/regionserver/RegionServerAbortedException.html"]}
{"id":"hbase-1015","summary":"MasterNotRunningException Is Thrown When the HBase Master Is Not Running","description":"Please check the HBase Active Master logs if there are any stuck procedures, regions in transition, or old ProcWALs holding the assignment. Based on the relevant error message, also check the ZooKeeper logs.","keywords":["org.apache.hadoop.hbase.MasterNotRunningException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/class-use/MasterNotRunningException.html"]}
{"id":"hbase-1016","summary":"Please Check if There Are Too Many Requests in the System, and Need to Increase the Region Handler Count: hbase.regionserver.handler.count","description":"Call queue is full on 0.0.0.0.16020? Returned to clients when their request was dropped because the call queue was too big to accept a new call. Clients should retry upon receiving it.","keywords":["org.apache.hadoop.hbase.CallQueueTooBigException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/CallQueueTooBigException.html"]}
{"id":"hbase-1017","summary":"Please Check Your Region Server Logs","description":"This condition arises when ZooKeeper has informed the HBase master that the region server is dead, and the region server is still trying to connect to ZooKeeper on the same connection. A likely cause of this is GC pauses. This exception is thrown by the master when a region server reports and is already being processed as dead. This can happen when a region server loses its session but didn't figure it out yet.","keywords":["org.apache.hadoop.hbase.YouAreDeadException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/YouAreDeadException.html"]}
{"id":"hbase-1018","summary":"The Exception Occurs When a Client Is Trying to Create a Table That Already Exists","description":"Thrown when a table exists but should not.","keywords":["org.apache.hadoop.hbase.TableExistsException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/TableExistsException.html"]}
{"id":"hbase-1019","summary":"This Exception Occurs When You Are Trying to Perform an Action on a Table That HBase Expects to Be Offline","description":"Thrown if a table should be offline but is not. For example, if while dropping a table, and the table is enabled, it will throw the same exception.","keywords":["org.apache.hadoop.hbase.TableNotDisabledException"],"knowledge_center_links":["https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/TableNotDisabledException.html"]}
{"id":"hbase-1020","summary":"The Following Exception Can Occur When We Are Trying to Work or Assign a Region That Does Not Exist in Meta","description":"Thrown when we are asked to operate on a region we know nothing about. Please check the region ID.","keywords":["org.apache.hadoop.hbase.UnknownRegionException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/UnknownRegionException.html"]}
{"id":"hbase-1021","summary":"ERROR: org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException","description":"Exception thrown when the found snapshot info from the file system is not valid. Please check snapshot integrity using the command: 'hbase snapshot info -snapshot <snapshot-name> -files'.","keywords":["org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/snapshot/CorruptedSnapshotException.html"]}
{"id":"hbase-1022","summary":"The Following Warning Can Occur When the BucketCache Size Is Less and the Block to Be Cached Is Not Able to Fit into the Cache","description":"This class is used to allocate a block with a specified size and free the block when evicting. It manages an array of buckets, each bucket is associated with a size and caches elements up to this size. For a completely empty bucket, this size could be re-specified dynamically. Please see if the BucketCache size can be extended.","keywords":["org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocatorException"],"knowledge_center_links":["https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/io/hfile/bucket/BucketAllocatorException.html"]}
{"id":"hbase-1023","summary":"The Error Occurs on the Client Side If the Region Where the Request Is Made Is on a Region That Is Not Yet Active","description":"Indicates that we're trying to connect to an already known as dead server. We will want to retry: we're getting this because the region location was wrong, or because the server just died, in which case the retry loop will help us to wait for the regions to recover. Please retry your operation, you may have to wait for the regions to be assigned to other region servers for the request to work.","keywords":["org.apache.hadoop.hbase.ipc.FailedServerException"],"knowledge_center_links":["https://hbase.apache.org/2.0/apidocs/org/apache/hadoop/hbase/ipc/FailedServerException.html"]}