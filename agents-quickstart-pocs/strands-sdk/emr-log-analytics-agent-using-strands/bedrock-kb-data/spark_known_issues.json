{"id":"spark-1001","summary":"Too Many Fetch Failures","description":"The presence of 'Too many fetch-failures' or 'Error reading task output' error messages in step or task attempt logs indicates that the running task is dependent on the output of another task. This often occurs when a reduce task is queued to execute and requires the output of one or more map tasks, and the output is not yet available.","keywords":["org.apache.spark.shuffle.FetchFailedException","FetchFailedException","org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException"],"knowledge_center_links":["https://aws.amazon.com/blogs/big-data/spark-enhancements-for-elasticity-and-resiliency-on-amazon-emr/","https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-troubleshoot-error-resource-1.html"]}
{"id":"spark-1002","summary":"RpcTimeoutException","description":"When the EMR Cluster nodes are under heavy load, RpcTimeoutException can occur. Timeout exceptions will occur when the executor is under memory constraint or facing OOM issues while processing data, impacting GC and causing further delay. Increase spark.executor.heartbeatInterval or specify a longer spark.network.timeout period.","keywords":["org.apache.spark.rpc.RpcTimeoutException","Issue communicating with driver in heartbeater org.apache.spark.rpc.RpcTimeoutException"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-troubleshoot-failed-spark-jobs"]}
{"id":"spark-1003","summary":"SPARK AnalysisException Detected","description":"AnalysisException is thrown when a query fails to analyze, usually because the query itself is invalid.","keywords":["org.apache.spark.sql.AnalysisException"],"knowledge_center_links":[""]}
{"id":"spark-1004","summary":"Pyspark AnalysisException Detected","description":"Pyspark AnalysisException is thrown when a query fails to analyze, usually because the query itself is invalid.","keywords":["pyspark.sql.utils.AnalysisException"],"knowledge_center_links":[""]}
{"id":"spark-1005","summary":"Spark App Failed with FileDownloadException","description":"FileDownloadException is the generic error; it could be due to Amazon S3 issues like access denied, KMS keys, or EMRFS credentials. The root cause can be identified by observing the error stack trace and the corresponding exception linked to FileDownloadException.","keywords":["org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path"],"knowledge_center_links":[""]}
{"id":"spark-1006","summary":"Generic PythonException Detected","description":"Please look for 'Traceback (most recent call last)'.","keywords":["org.apache.spark.api.python.PythonException"],"knowledge_center_links":[""]}
{"id":"spark-1007","summary":"A Generic SparkException","description":"A generic error was found. Check the log data for more details.","keywords":["org.apache.spark.SparkException"],"knowledge_center_links":[""]}
{"id":"spark-1008","summary":"Spark History Events UI Issues Detected","description":"On-cluster Spark History Server events are not accessible if you save the Spark events in an Amazon S3 bucket on Amazon EMR releases before 6.3 and 5.30. The Spark History Server in these Amazon EMR versions does not have the emrfs-hadoop-assembly JAR file required to access Amazon S3 buckets. Without this JAR file, you receive the following error when trying to access Spark History Server events.","keywords":["java.lang.ClassNotFoundException: Class com.amazon.ws.emr.hadoop.fs.EmrFileSystem not found"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-view-spark-history-events"]}
{"id":"spark-1009","summary":"Failed Spark-Submit Job with Possible Executor Memory Issue","description":"Indicates a memory problem. Please increase the executor memory using the --executor-memory option or spark.executor.memory in the Spark configuration.","keywords":["java.lang.IllegalArgumentException: Executor memory","Please increase executor memory using the --executor-memory option or spark.executor.memory in Spark configuration"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-view-spark-history-events"]}
{"id":"spark-1010","summary":"ClassNotFoundException Detected","description":"The Spark job cannot find the relevant files in the classpath. When this happens, the class loader picks up only the JAR files that exist in the location that you specified in your configuration. Check the stack trace to find the name of the missing class. Then, add the path of your custom JAR (containing the missing class) to the Spark classpath. You can do this while the cluster is running, when you launch a new cluster, or when you submit a job.","keywords":["java.lang.ClassNotFoundException"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-spark-classnotfoundexception"]}
{"id":"spark-1011","summary":"Job Failure Detected Due to Stage or Task Failure","description":"In Spark, stage failures happen when there is a problem with processing a Spark task. These failures can be caused by hardware issues, incorrect Spark configurations, or code problems.","keywords":["org.apache.spark.SparkException: Job aborted due to stage failure: Task","ExecutorLostFailure","failed 4 times, most recent failure: Lost task"],"knowledge_center_links":[""]}
{"id":"spark-1012","summary":"Node Issues Detected","description":"This error indicates that a Spark task failed because a node terminated or became unavailable. There are many possible causes of this error. The following resolution covers these common root causes: high disk utilization, using Spot Instances for cluster nodes, and aggressive auto-scaling policies.","keywords":["Lost task","exited caused by one of the running tasks Reason: Slave lost"],"knowledge_center_links":["https://repost.aws/knowledge-center/executorlostfailure-slave-lost-emr"]}
{"id":"spark-1013","summary":"Spark Applications on EMR Notebook Failures Detected","description":"Spark applications running on the EMR notebook failing with a fatal error occur due to insufficient cluster resources, Livy server issues, or session timeout. Resolved by increasing driver memory, increasing the Livy server timeout, or restarting the kernel.","keywords":["The code failed because of a fatal error"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-troubleshot-spark-apps"]}
{"id":"spark-1014","summary":"Containers Got Killed with Exit Code 137","description":"When a container (Spark executor) runs out of memory, YARN automatically kills it. This causes the 'Container killed on request. Exit code is 137' error. These errors can happen in different job stages, both in narrow and wide transformations. YARN containers can also be killed by the OS oom_reaper when the OS is running out of memory, causing this error.","keywords":["Diagnostics: Container killed on request. Exit code is 137"],"knowledge_center_links":["https://repost.aws/knowledge-center/container-killed-on-request-137-emr"]}
{"id":"spark-1015","summary":"Amazon S3 Throttling Detected","description":"This error occurs when the Amazon Simple Storage Service (Amazon S3) request rate for your application exceeds the typically sustained rates of over 5,000 requests per second, and Amazon S3 internally optimizes performance.","keywords":["com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Slow Down (Service: Amazon S3"],"knowledge_center_links":["https://aws.amazon.com/blogs/big-data/best-practices-to-optimize-data-access-performance-from-amazon-emr-and-aws-glue-to-amazon-s3/","https://repost.aws/knowledge-center/emr-s3-503-slow-down"]}
{"id":"spark-1016","summary":"Disk Issues Detected","description":"Spark uses local disks on the core and task nodes to store intermediate data. If the disks run out of space, then the job fails with a 'no space left on device' error. Use one of the following methods to resolve this error: add more Amazon Elastic Block Store (Amazon EBS) capacity, add more Spark partitions, or use a bootstrap action to dynamically scale up storage on the core and task nodes. For more information and an example bootstrap action script, see 'Dynamically scale up storage on Amazon EMR clusters'.","keywords":["No space left on device"],"knowledge_center_links":["https://repost.aws/knowledge-center/no-space-left-on-device-emr-spark"]}
{"id":"spark-1017","summary":"Container Got Killed by YARN for Exceeding Memory Limits","description":"The root cause and the appropriate solution for this error depend on your workload. You might have to try each of the following methods, in the following order, to troubleshoot the error: increase memory overhead, reduce the number of executor cores, increase the number of partitions, and increase driver and executor memory.","keywords":["Container killed by YARN for exceeding memory limits"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-spark-yarn-memory-limit"]}
{"id":"spark-1018","summary":"ChunkFetchFailureException","description":"The root cause is usually because the Executor (with the BlockManager for requested shuffle blocks) is lost and no longer available due to the node with shuffle data being in a decommissioned state due to unhealthy conditions, spot instance issues, or scale-down events.","keywords":["org.apache.spark.network.client.ChunkFetchFailureException"],"knowledge_center_links":[""]}
{"id":"spark-1019","summary":"Spark Job in Notebook or Studio Failed with 'Session Not Found' Error","description":"This error usually happens when you keep the session running until it times out when running a Spark job through Livy using a Jupyter Notebook or Studio. The default value for livy.server.session.timeout is 1 hour. To resolve, increase the value of the livy.server.session.timeout property in /etc/livy/conf/livy.conf.","keywords":["404 from","with error payload: session 0 not found"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-session-not-found-http-request-error"]}
{"id":"spark-1020","summary":"Broadcast Join Failures","description":"This error occurs when a broadcast join takes too long, and the job times out during the broadcast join phase. Possible causes are that the broadcast was too large or the network was slow, which caused the broadcast to timeout. The solution is to increase the timeout for the broadcast (--conf spark.sql.broadcastTimeout=600s) or totally disable the broadcast join (--conf spark.sql.autoBroadcastJoinThreshold=-1).","keywords":["org.apache.spark.SparkException: Could not execute broadcast in 300 secs","You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1"],"knowledge_center_links":[""]}
{"id":"spark-1021","summary":"Class org.apache.hadoop.fs.s3a.S3AFileSystem Not Found","description":"The s3a filesystem is not recommended, and the suggestion is to leverage EMRFS (i.e., using the s3:// scheme instead of the s3a:// scheme).","keywords":["java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found"],"knowledge_center_links":["https://repost.aws/questions/QUXO0H1bgKTWqiEYLEE-msAQ/executing-hive-create-table-in-spark-sql-java-lang-classnotfoundexception-class-org-apache-hadoop-fs-s3a-s3afilesystem-not-found"]}
{"id":"spark-1022","summary":"Spark Application Failed with 'Not Found' Error","description":"This error indicates that an Amazon S3 file or path does not exist. The issue could be due to a missing file in Amazon S3. Before launching your application, confirm that you correctly entered the Amazon S3 path and that the path exists. This issue can occur when other applications or accounts are accessing the same Amazon S3 files. It's possible that the file or path was deleted by another application or account. Before launching your application, check if other applications or accounts are actively accessing the same Amazon S3 path.","keywords":["com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: Not Found","Service: Amazon S3; Status Code: 404; Error Code: 404 Not Found;"],"knowledge_center_links":["https://repost.aws/knowledge-center/emr-s3-404-not-found"]}
{"id":"spark-1023","summary":"Broadcast Join Limit Exceeded","description":"8 GB is the limit, and it cannot be exceeded for a broadcast join in Spark. To resolve, disable the broadcast join (spark.sql.autoBroadcastJoinThreshold = -1) or avoid using broadcast hints in code with large data, or increase spark.sql.autoBroadcastJoinThreshold.","keywords":["org.apache.spark.SparkException: Cannot broadcast the table that is larger than"],"knowledge_center_links":[""]}
{"id":"spark-1024","summary":"Spark App Failed with 'AM Container Launch Exit Code: 13'","description":"Due to an issue with the AM Container launch, your Spark app has failed with Exit code: 13, which is a more generic exception. To mitigate this issue, packages in your applications must be compatible with the Spark version, application modules/dependencies should be deployed alongside the driver on the core nodes, and ensure the Python code's Spark context does not point to local or override the cluster mode.","keywords":["AM Container for app attempt","exited with exitCode: 13","Exception from container-launch. Exit code: 13"],"knowledge_center_links":["https://repost.aws/questions/QUFLPLPESJREqZ-yyZe92pjA/spark-submit-is-failing-in-cluster-mode-for-pyspark-application"]}
{"id":"spark-1025","summary":"MultiObjectDeleteException in Spark Job","description":"It is caused when BATCH.DELETE.OBJECT is performed. To delete a whole folder, EMRFS will list all the objects in the folder and use the deleteAll() to delete all the keys in one Amazon S3 request. The root cause can be many of the following reasons, such as Amazon S3 throttling, Amazon S3 access denied, issues with the s3a filesystem, or any known issues in a specific EMR version.","keywords":["com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.MultiObjectDeleteException: One or more objects could not be deleted"],"knowledge_center_links":[""]}
{"id":"spark-1026","summary":"Unable to Query a Table in Spark SQL Using AWS Lake Formation","description":"This issue is encountered when there is an access issue when trying to access the AWS Glue Data Catalog managed by AWS Lake Formation access control.","keywords":["org.apache.spark.sql.catalyst.analysis.AccessControlException"],"knowledge_center_links":["https://repost.aws/knowledge-center/glue-insufficient-lakeformation-permissions"]}
{"id":"spark-1027","summary":"Total Size of Serialized Results of Tasks Is Bigger Than spark.driver.maxResultSize","description":"The spark.driver.maxResultSize property defines the limit of the total size of serialized results of all partitions for each Spark action (e.g., collect) in bytes. This issue can also occur when broadcasting a table that is too big. Spark downloads all the rows for a table that needs to be broadcasted to the driver before it starts shipping to the executors. If broadcasting a table that is larger than spark.driver.maxResultSize causes such issues, increasing spark.driver.maxResultSize helps, but if you set a high limit, an OutOfMemoryError can occur.","keywords":["Job aborted due to stage failure: Total size of serialized results of tasks is bigger than spark.driver.maxResultSize"],"knowledge_center_links":[""]}
{"id":"spark-1028","summary":"Block Not Found Exceptions","description":"Spark shuffle blocks are managed by the NodeManager. If there are any issues with the NodeManager like an OutOfMemoryError or restart, this issue occurs. Also, check if there are any issues with the node with the corresponding NodeManager.","keywords":["org.apache.spark.storage.BlockNotFoundException","org.apache.spark.SparkException: Failed to get broadcast_piece"],"knowledge_center_links":[""]}
{"id":"spark-1029","summary":"Spark Job Failed with Exit Code 1","description":"The container was stopped due to an application error. Exit Code 1 indicates that a container shut down, either because of an application failure, issues with user code or script. It can also occur if there are incorrect arguments in the spark-submit command in cluster mode.","keywords":["org.apache.spark.SparkUserAppException: User application exited with 1"],"knowledge_center_links":[""]}
{"id":"spark-1030","summary":"Application Failed with Upgrade Exception","description":"This is a limitation on Spark version 3.0 with the old date and timestamp value results. When you read or write dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z into Parquet files, the resultant values are erroneous. Set spark.sql.legacy.parquet.datetimeRebaseModeInWrite to LEGACY and spark.sql.legacy.parquet.datetimeRebaseModeInWrite to CORRECTED.","keywords":["org.apache.spark.SparkUpgradeException","You may get a different result due to the upgrading of Spark 3.0"],"knowledge_center_links":[""]}